{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install visionlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'VisionLens'...\n",
      "remote: Enumerating objects: 216, done.\u001b[K\n",
      "remote: Counting objects: 100% (216/216), done.\u001b[K\n",
      "remote: Compressing objects: 100% (147/147), done.\u001b[K\n",
      "remote: Total 216 (delta 110), reused 167 (delta 64), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (216/216), 12.58 MiB | 17.60 MiB/s, done.\n",
      "Resolving deltas: 100% (110/110), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /kaggle/working/*\n",
    "!git clone https://github.com/SKT27182/VisionLens.git\n",
    "!mv  /kaggle/working/VisionLens/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops==0.8.0\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install einops==0.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import einops\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "\n",
    "from visionlens.models import InceptionV1\n",
    "from visionlens.objectives import objective_wrapper\n",
    "from visionlens.optimize import Visualizer\n",
    "from visionlens import objectives\n",
    "from visionlens.images import get_images\n",
    "from visionlens.utils import device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionV1(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransfer:\n",
    "\n",
    "    def __init__(self, model, content_image: Union[str, torch.Tensor], style_image: Union[str, torch.Tensor]):\n",
    "        self.model = model\n",
    "        self.content_image = content_image if isinstance(content_image, torch.Tensor) else self.load_image(content_image)\n",
    "        self.style_image = style_image if isinstance(style_image, torch.Tensor) else self.load_image(style_image)\n",
    "\n",
    "        self.TRANSFER_INDEX = 0\n",
    "        self.CONTENT_INDEX = 1\n",
    "        self.STYLE_INDEX = 2\n",
    "\n",
    "    def load_image(self, file_path: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert an image to a tensor.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the image file.\n",
    "\n",
    "        Returns:\n",
    "            Tensor representation of the image of shape (H, W, C).\n",
    "        \"\"\"\n",
    "        img = plt.imread(file_path)\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        return img\n",
    "\n",
    "    def get_style_transfer_params(self, content_image, style_image, decorrelate=True, fft=True):\n",
    "        \"\"\"\n",
    "        Generates parameters and a function for style transfer.\n",
    "\n",
    "        Args:\n",
    "            content_image (numpy.ndarray): The content image to be used for style transfer.\n",
    "            style_image (numpy.ndarray): The style image to be used for style transfer.\n",
    "            decorrelate (bool, optional): Whether to decorrelate the images. Defaults to True.\n",
    "            fft (bool, optional): Whether to use FFT for image processing. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - params: Parameters for the style transfer.\n",
    "                - inner (function): A function that returns a tensor stack of style transfer input, content input, and style input.\n",
    "        \"\"\"\n",
    "\n",
    "        content_h, content_w = content_image.shape[:2]  # assume we use content_image.shape\n",
    "        params, image = get_images(content_h, content_w, decorrelate=decorrelate, fft=fft)\n",
    "\n",
    "        def inner():\n",
    "            style_transfer_input = image()[0]\n",
    "\n",
    "            content_input = (\n",
    "                torch.tensor(einops.rearrange(content_image, \"h w c -> c h w\").float().to(device))\n",
    "            )\n",
    "            style_input = (\n",
    "                torch.tensor(\n",
    "                    einops.rearrange(\n",
    "                        style_image[:content_h, :content_w, :], \"h w c -> c h w\"\n",
    "                    )\n",
    "                )\n",
    "                .float()\n",
    "                .to(device)\n",
    "            )\n",
    "            return torch.stack([style_transfer_input, content_input, style_input])\n",
    "\n",
    "        return params, inner\n",
    "\n",
    "    @staticmethod\n",
    "    def gram_matrix(features, normalize=True):\n",
    "        C, H, W = features.shape\n",
    "        # Flatten the features to compute the gram matrix\n",
    "        features = einops.rearrange(features, \"c h w -> c (h w)\")\n",
    "        # Compute the gram matrix, which is of shape (C, C)\n",
    "        gram = torch.einsum(\"cl, dl -> cd\", features, features)\n",
    "\n",
    "        if normalize:\n",
    "            gram = gram / (H * W)\n",
    "        return gram\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_L1_loss(x, y):\n",
    "        \"\"\"\n",
    "            Compute the mean L1 loss between two tensors.\n",
    "\n",
    "            Args:\n",
    "                x (torch.Tensor): The first tensor.\n",
    "                y (torch.Tensor): The second tensor.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The mean L1 loss.\n",
    "            \"\"\"\n",
    "        return torch.mean(torch.abs(x - y))\n",
    "\n",
    "    @objective_wrapper\n",
    "    def get_activations_difference(\n",
    "        self,\n",
    "        layer_names,\n",
    "        difference_to,\n",
    "        loss_type=None,\n",
    "        obj_name=\"activation_difference\",\n",
    "        transform_f=None,\n",
    "    ):\n",
    "\n",
    "        obj_name = f\"{obj_name}_activations_difference\"\n",
    "\n",
    "        loss_type = StyleTransfer.mean_L1_loss if loss_type is None else loss_type\n",
    "\n",
    "        def get_activation_loss(act_dict):\n",
    "\n",
    "            image_activations = [\n",
    "                act_dict(layer_name)[difference_to] for layer_name in layer_names\n",
    "            ]\n",
    "\n",
    "            if transform_f is not None:\n",
    "                image_activations = [transform_f(act) for act in image_activations]\n",
    "\n",
    "            optimization_activations = [\n",
    "                act_dict(layer_name)[self.TRANSFER_INDEX] for layer_name in layer_names\n",
    "            ]\n",
    "\n",
    "            if transform_f is not None:\n",
    "                optimization_activations = [transform_f(act) for act in optimization_activations]\n",
    "\n",
    "            losses = [loss_type(optimization_act, image_act) for optimization_act, image_act in zip(optimization_activations, image_activations)]\n",
    "\n",
    "            return torch.stack(losses).sum()\n",
    "\n",
    "        return get_activation_loss, obj_name\n",
    "\n",
    "    def style_transfer(\n",
    "        self,\n",
    "        style_layers,\n",
    "        content_layers,\n",
    "        content_weight=200,\n",
    "        style_weight=1,\n",
    "        decorrelate=True,\n",
    "        fft=True,\n",
    "        threshold=(5, 50, 512), \n",
    "        ** kwargs,\n",
    "    ):\n",
    "        STYLE_LAYERS = style_layers\n",
    "        CONTENT_LAYERS = content_layers\n",
    "\n",
    "        param_f = lambda: self.get_style_transfer_params(self.content_image, self.style_image, decorrelate=decorrelate, fft=fft)\n",
    "\n",
    "        content_obj = self.get_activations_difference(CONTENT_LAYERS, difference_to=self.CONTENT_INDEX, obj_name=\"content_loss\")\n",
    "\n",
    "        style_obj = self.get_activations_difference(\n",
    "            STYLE_LAYERS, transform_f=self.gram_matrix, difference_to=self.STYLE_INDEX, obj_name=\"style_loss\"\n",
    "        )\n",
    "\n",
    "        objective = content_weight * content_obj + style_weight * style_obj\n",
    "\n",
    "        viz = Visualizer(self.model, objective)\n",
    "        images = viz.visualize(param_f, lr=0.1, threshold=threshold, **kwargs)\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_LAYERS = [\n",
    "    \"conv2d2\",\n",
    "    \"mixed3a\",\n",
    "    \"mixed4a\",\n",
    "    \"mixed4b\",\n",
    "    \"mixed4c\",\n",
    "]\n",
    "\n",
    "CONTENT_LAYERS = [\n",
    "    \"mixed3b\",\n",
    "]\n",
    "\n",
    "\n",
    "content_image_pth = \"images/transfer_big_ben.png\"\n",
    "style_image_pth = \"images/transfer_vangogh.png\"\n",
    "\n",
    "style_transfer = StyleTransfer(\n",
    "    model, content_image=content_image_pth, style_image=style_image_pth\n",
    ")\n",
    "images = style_transfer.style_transfer(\n",
    "    STYLE_LAYERS, CONTENT_LAYERS, content_weight=200, style_weight=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visionlens.img_utils import display_images_in_table\n",
    "display_images_in_table(images[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_LAYERS = [\n",
    "    \"conv2d2\",\n",
    "    \"mixed3a\",\n",
    "    \"mixed4a\",\n",
    "    \"mixed4b\",\n",
    "    \"mixed4c\",\n",
    "]\n",
    "\n",
    "CONTENT_LAYERS = [\n",
    "    \"mixed3b\",\n",
    "]\n",
    "\n",
    "\n",
    "content_image_pth = \"images/transfer_big_ben.png\"\n",
    "style_image_pth = \"images/transfer_picasso.png\"\n",
    "\n",
    "style_transfer = StyleTransfer(\n",
    "    model, content_image=content_image_pth, style_image=style_image_pth\n",
    ")\n",
    "images = style_transfer.style_transfer(\n",
    "    STYLE_LAYERS, CONTENT_LAYERS, content_weight=200, style_weight=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
